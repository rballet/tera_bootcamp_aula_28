{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERA - DSCSP - Aula 28\n",
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introdução\n",
    "\n",
    "Na aula sobre clustering nós utilizamos diversos algoritmos de clustering e redução de dimensionalidade para conseguir encontrar relações de proximidade entre documentos. Entretanto, apesar de algoritmos como o PCA conseguirem representar reduzidamente o nosso conjunto de documentos, nós não conseguíamos interpretar o resultado obtido. Isso acontece porque o PCA encontra novos vetores de features que são combinações lineares do conjunto de palavras existentes. Esse fator pode não ser um problema se o que se deseja é apenas encontrar clusters sem interpretações mais profundas. Entretanto, muitas vezes gostaríamos de entender o racional por trás da geração dos clusters. Ainda mais, as vezes gostaríamos de reduzir um documento a um conjunto de palavras-chave que podem \"resumir\" o nosso documento e agrupá-las em **tópicos**. E é exatamente esse o objetivo dessa aula.\n",
    "\n",
    "A área de topic analysis é de grande importância para Machine Learning, ou mais especificamente a área de Data Mining. A utilização de tópicos nos permite ter uma melhor e mais compacta representação dos nossos dados, principalmente quando temos um conjunto extenso de dados e atributos (features). \n",
    "\n",
    "Utilizar topic analysis em Natural Language Processing (NLP) é algo bem intuitivo. Nós naturalmente fazemos isso quando queremos organizar textos (documentos) em diferentes categorias, ou temas. Por exemplo, nós podemos ler artigos do Google News e dizer facilmente que um determinado artigo tem o tema \"esporte\", ou o tema \"política\". Nosso trabalho em topic analysis é o de conseguir desenvolver algoritmos de Machine Learning que possam encontrar automaticamente esses tópicos, ou temas, por nós.\n",
    "\n",
    "Vamos definir a seguir alguns termos importantes que serão utilizados daqui por diante:\n",
    "- **documentos**: são conjuntos de atributos (normalmente palavras) associadas a amostras de uma população (ex: artigos do wikipedia, texto de um livro etc)\n",
    "- **atributos** (features): é o conjunto de variáveis observadas. Normalmente é um conjunto de palavras que compõe o vocabulário utilizado.\n",
    "- **variável latente**: variáveis, ou atributos, implícitas no sistema. No nosso caso, podem representar os tópicos dos documentos.\n",
    "- **vetor de atributos**: é a representação de um determinado documento a partir dos atributos pertencentes a ele\n",
    "- **matriz de frequência de termos**: é o empilhamento de diversos vetores de atributos associados a cada documento. Cada documento representa uma linha na matriz, enquanto as colunas representam os atributos dos documentos.\n",
    "\n",
    "<img src=\"./imagens/mat_freq.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar a praticar!\n",
    "\n",
    "Primeiro vamos realizar os imports necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports usados no curso\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs: Lembre-se de colocar os datasets baixados dentro de uma pasta \"`datasets`\" na raiz da pasta clonada do repositório da aula!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pasta contendo os dados:\n",
    "DATASET_FOLDER = '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nós vamos começar utilizando o conhecido dataset de artigos do Wikipedia. Para começar leve, vamos replicar uma parte do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>artigo</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black Sabbath</td>\n",
       "      <td>Black Sabbath are an English rock band, formed...</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lymphoma</td>\n",
       "      <td>Lymphoma is a type of blood cancer that occurs...</td>\n",
       "      <td>Sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hepatitis C</td>\n",
       "      <td>Hepatitis C is an infectious disease affecting...</td>\n",
       "      <td>Sickness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HTTP cookie</td>\n",
       "      <td>A cookie, also known as an HTTP cookie, web co...</td>\n",
       "      <td>Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global warming</td>\n",
       "      <td>Global warming is the rise in the average temp...</td>\n",
       "      <td>Global_Warming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           titulo                                             artigo  \\\n",
       "0   Black Sabbath  Black Sabbath are an English rock band, formed...   \n",
       "1        Lymphoma  Lymphoma is a type of blood cancer that occurs...   \n",
       "2     Hepatitis C  Hepatitis C is an infectious disease affecting...   \n",
       "3     HTTP cookie  A cookie, also known as an HTTP cookie, web co...   \n",
       "4  Global warming  Global warming is the rise in the average temp...   \n",
       "\n",
       "          cluster  \n",
       "0           Music  \n",
       "1        Sickness  \n",
       "2        Sickness  \n",
       "3        Internet  \n",
       "4  Global_Warming  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Abra o dataset\n",
    "df_wiki = pd.read_csv(os.path.join(DATASET_FOLDER, 'wikipedia_dataset_60.csv'), sep=',', names=['titulo', 'artigo', 'cluster'])\n",
    "\n",
    "df_wiki.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre que os clusters indicados foram feitos apenas para fins didáticos. Na maioria das vezes nós não teremos informações a respeito da relação entre documentos. Realizar esse processo seria trabalhoso demais para a maioria das situações envolvendo Machine Learning. E é exatamente esse tipo de trabalho que queremos automatizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número aproximado de clusters\n",
    "n_clusters = len(pd.unique(df_wiki['cluster']))\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Sabbath: \n",
      "Black Sabbath are an English rock band, formed in Birmingham in 1968, by guitarist Tony Iommi, bassist Geezer Butler, singer Ozzy Osbourne, and drummer Bill Ward. The band has since experienced multiple line-up changes, with Tony Iommi the only constant presence in the band through the years. Originally formed in 1968 as a heavy blues rock band named Earth, the band began incorporating occult themes with horror-inspired lyrics and tuned-down guitars. Despite an association with occult and horror themes, Black Sabbath also composed songs dealing with social instability, political corruption, the dangers of drug abuse and apocalyptic prophecies of the horrors of war. Osbourne's heavy drug use led to his dismissal from the band in 1979. He was replaced by former Rainbow vocalist Ronnie James Dio. After a few albums with Dio's vocals and songwriting collaborations, Black Sabbath endured a revolving line-up in the 1980s and '90s that included vocalists Ian Gillan, Glenn Hughes, Ray Gillen a (...)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de um artigo:\n",
    "# Vamos mostrar apenas 1000 caracteres\n",
    "print(\"Black Sabbath: \\n{} (...)\".format(df_wiki[df_wiki.titulo=='Black Sabbath']['artigo'].values[0][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar agora um embedding para esse texto. Nós utilizaremos uma abordagem de Bag-of-Words (BOW) para esse problema. Mais especificamente, vamos utilizar o Tf-Idf com um tamanho de vocabulário de 15000 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 15000)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importe o método TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crie o vetor de embeddings Tf-Idf\n",
    "# Vamos definir o número máximo de palavras do nosso dicionário (número de dimensões)\n",
    "# igual a 15000. \n",
    "# Também utilizamos um corte em termos muito frequentes em um dado\n",
    "# documento: max_df=0.8\n",
    "# Igualmente, realizamos um corte de termos muito pouco frequentes: min_df=0.01\n",
    "# O parâmetro sublinear_tf utiliza a função 1+log(tf) em vez de uma função linear\n",
    "# para calcular o peso da frequência de cada termo. Isso permite uma função mais \"suave\"\n",
    "# use_idf: Utiliza o inverso da frequência do documento para recriar os pesos da matriz\n",
    "tfidf = TfidfVectorizer(max_df=0.8, min_df=0.01, max_features=15000, sublinear_tf=True, use_idf=True)\n",
    "\n",
    "# Precisamos extrair os artigos e títulos do dataframe\n",
    "titles = df_wiki['titulo'].values\n",
    "articles = df_wiki['artigo'].values\n",
    "\n",
    "# Aplique a transformação nos artigos\n",
    "X = tfidf.fit_transform(articles)\n",
    "\n",
    "# Tamanho do dataset\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja algumas palavras que fazem parte do nosso vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opined',\n",
       " 'tipshallen',\n",
       " 'disapproving',\n",
       " 'donated',\n",
       " 'currently',\n",
       " 'stratosphere',\n",
       " 'boycott',\n",
       " 'scooter',\n",
       " 'vasoconstriction',\n",
       " 'reductions',\n",
       " 'valentine',\n",
       " 'gone',\n",
       " 'hagel',\n",
       " 'quickly',\n",
       " 'monica',\n",
       " 'born',\n",
       " 'lucia',\n",
       " 'adoptions',\n",
       " 'sexual',\n",
       " 'colossus']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf.vocabulary_.keys())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que temos 58 documentos (número de linhas) por 15000 atributos (palavras - colunas). Essa matriz é o que definimos anteriormente por **matriz de frequência de palavras**.\n",
    "\n",
    "Como temos uma dimensão muito elevada, nós podemos realizar algumas alternativas para reduzir a dimensionalidade. A primeira alternativa é o PCA, mas, como já dissemos anteriormente, ele não nos permite ter uma interpretação dos resultados. Portanto, precisamos de outras alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "O [NMF](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) é um algoritmo poderoso (apesar de relativamente simples) para encontrar tópicos em um conjunto de documentos e features. Ele se baseia em um processo de decomposição de matrizes para criar uma representação adequada da matriz de frequência de palavras (denotado por **A**). Mais especificamente, o NMF decompõe a matriz de frequência de palavras em duas: a primeira é a matriz de pesos (chamada de **W**), com as linhas representando os documentos e as colunas indicando os tópicos; e a segunda é a matriz de atributos (chamada de **H**), com as linhas indicando os tópicos e as colunas os atributos. O número de tópicos é definido antecipadamente e é fixo.\n",
    "\n",
    "<img src=\"../imagens/nmf_draw.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "As duas matrizes são formadas a partir de um processo iterativo de otimização (veja esse [link](http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf) para mais detalhes) com o objetivo de reconstruir fielmente a matriz **A**. Entretanto, para esse fim, a matriz **A** não pode possuir entradas negativas.\n",
    "\n",
    "Vamos aplicar esse método no problema dos artigos do Wikipedia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeiro, importe o módulo NMF do scikit-learn\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Precisamos criar a instância do NMF\n",
    "# Temos que definir um número de componentes para o NMF\n",
    "# Como temos 6 clusters, vamos escolher n_components=6\n",
    "nmf = NMF(n_components=6)\n",
    "\n",
    "# Agora vamos utilizar os mesmos atributos fit, transform ou fit_transform\n",
    "# que já conhecemos do universo do sklearn\n",
    "W_nmf = nmf.fit_transform(X)\n",
    "\n",
    "# Vamos ver qual é a dimensão de W_nmf\n",
    "W_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas se manteve em 58, que é o número de documentos (artigos) que nós temos, e o número de colunas se transformou em 6, que é o número de tópicos que nós escolhemos. Essa matriz gerada representa a matriz **W** (matriz de pesos) da fatoração de matrizes.\n",
    "\n",
    "Vamos agora achar a matriz **H** que representa a matriz de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 15000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas é igual ao número de tópicos e o número de colunas representa o número de palavras no nosso vocabulário. Cada linha da matriz é definida como um componente (assim como o PCA possui os componentes principais) que está associado a um tópico específico. Entretanto, diferentemente do PCA, nós podemos associar cada componente a um conjunto específico de palavras. Vamos verificar abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topico 0:\n",
      "----------\n",
      "film       0.228699\n",
      "she        0.215295\n",
      "starred    0.189635\n",
      "her        0.185833\n",
      "award      0.161151\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "Topico 1:\n",
      "----------\n",
      "treatment    0.162274\n",
      "disease      0.138570\n",
      "symptoms     0.134004\n",
      "infection    0.130332\n",
      "blood        0.121010\n",
      "Name: 1, dtype: float64\n",
      "----------\n",
      "Topico 2:\n",
      "----------\n",
      "cup       0.131625\n",
      "scored    0.126175\n",
      "fifa      0.116823\n",
      "goals     0.112971\n",
      "team      0.108254\n",
      "Name: 2, dtype: float64\n",
      "----------\n",
      "Topico 3:\n",
      "----------\n",
      "climate       0.229255\n",
      "emissions     0.189170\n",
      "conference    0.131384\n",
      "greenhouse    0.125873\n",
      "change        0.120971\n",
      "Name: 3, dtype: float64\n",
      "----------\n",
      "Topico 4:\n",
      "----------\n",
      "users     0.161012\n",
      "web       0.158808\n",
      "search    0.152862\n",
      "google    0.147589\n",
      "user      0.141939\n",
      "Name: 4, dtype: float64\n",
      "----------\n",
      "Topico 5:\n",
      "----------\n",
      "album    0.161875\n",
      "band     0.143538\n",
      "song     0.110535\n",
      "tour     0.103392\n",
      "songs    0.089824\n",
      "Name: 5, dtype: float64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = [x[0] for x in sorted(tfidf.vocabulary_.items())]\n",
    "\n",
    "# Vamos criar um dataframe para visualizar\n",
    "components_df = pd.DataFrame(nmf.components_, columns=words)\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(6):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest())\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou da distribuição de palavras dentro de cada tópico? Acha que faz sentido com os temas principais dos artigos do Wikipedia? Cada um dos tópicos poderia ser associado a um cluster?\n",
    "\n",
    "Podemos ainda verificar quais são os tópicos principais de alguns artigos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.314543\n",
      "1    0.000000\n",
      "2    0.017131\n",
      "3    0.003643\n",
      "4    0.000000\n",
      "5    0.002090\n",
      "Name: Denzel Washington, dtype: float64\n",
      "\n",
      "0    0.005139\n",
      "1    0.495278\n",
      "2    0.000447\n",
      "3    0.000000\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Leukemia, dtype: float64\n",
      "\n",
      "0    0.016619\n",
      "1    0.000000\n",
      "2    0.513907\n",
      "3    0.000000\n",
      "4    0.000000\n",
      "5    0.035104\n",
      "Name: Neymar, dtype: float64\n",
      "\n",
      "0    0.024303\n",
      "1    0.000000\n",
      "2    0.035876\n",
      "3    0.043349\n",
      "4    0.376226\n",
      "5    0.011095\n",
      "Name: LinkedIn, dtype: float64\n",
      "\n",
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.009963\n",
      "3    0.000000\n",
      "4    0.019685\n",
      "5    0.593688\n",
      "Name: Arctic Monkeys, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Precisamos criar um dataframe para facilitar nossa vida\n",
    "df = pd.DataFrame(W_nmf, index=titles)\n",
    "\n",
    "print(df.loc['Denzel Washington'])\n",
    "print()\n",
    "print(df.loc['Leukemia'])\n",
    "print()\n",
    "print(df.loc['Neymar'])\n",
    "print()\n",
    "print(df.loc['LinkedIn'])\n",
    "print()\n",
    "print(df.loc['Arctic Monkeys'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que os artigos possuem tópicos coerentes com o que esperávamos!\n",
    "\n",
    "Vamos agrupar agora os artigos pelos tópicos principais de cada um deles e ver como eles tém relação com os clusters definidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          article  label\n",
      "10                                   Jessica Biel      0\n",
      "19                                 Angelina Jolie      0\n",
      "18                                     Mila Kunis      0\n",
      "38                                  Anne Hathaway      0\n",
      "5                            Catherine Zeta-Jones      0\n",
      "46                             Michael Fassbender      0\n",
      "7                               Denzel Washington      0\n",
      "50                               Jennifer Aniston      0\n",
      "25                                 Dakota Fanning      0\n",
      "12                                  Russell Crowe      0\n",
      "49                                    Tonsillitis      1\n",
      "31                                       Leukemia      1\n",
      "24                                          Fever      1\n",
      "52                                     Prednisone      1\n",
      "9                                            Gout      1\n",
      "8                                     Hepatitis B      1\n",
      "34                                    Doxycycline      1\n",
      "2                                     Hepatitis C      1\n",
      "1                                        Lymphoma      1\n",
      "21                                     Gabapentin      1\n",
      "48                                   Arsenal F.C.      2\n",
      "26                Colombia national football team      2\n",
      "28                  France national football team      2\n",
      "17                                         Neymar      2\n",
      "53                                  Franck Ribéry      2\n",
      "54              2014 FIFA World Cup qualification      2\n",
      "11                             Zlatan Ibrahimović      2\n",
      "6                                        Football      2\n",
      "44                              Cristiano Ronaldo      2\n",
      "36                                 Radamel Falcao      2\n",
      "37  Greenhouse gas emissions by the United States      3\n",
      "35                                   Nigel Lawson      3\n",
      "33                                        350.org      3\n",
      "41                                 Kyoto Protocol      3\n",
      "30                               Connie Hedegaard      3\n",
      "47       Nationally Appropriate Mitigation Action      3\n",
      "43  2007 United Nations Climate Change Conference      3\n",
      "15                                 Climate change      3\n",
      "4                                  Global warming      3\n",
      "45  2010 United Nations Climate Change Conference      3\n",
      "55                                 Alexa Internet      4\n",
      "51                                       LinkedIn      4\n",
      "57                                         Tumblr      4\n",
      "40                              Internet Explorer      4\n",
      "39                                       HTTP 404      4\n",
      "56                                  Google Search      4\n",
      "23                                  Social search      4\n",
      "22                                        Firefox      4\n",
      "3                                     HTTP cookie      4\n",
      "32                                         Sepsis      5\n",
      "29                          Red Hot Chili Peppers      5\n",
      "27                                     Nate Ruess      5\n",
      "20                                   Stevie Nicks      5\n",
      "16                                     The Wanted      5\n",
      "14                                 Arctic Monkeys      5\n",
      "13                                       Skrillex      5\n",
      "42                                   Chad Kroeger      5\n",
      "0                                   Black Sabbath      5\n"
     ]
    }
   ],
   "source": [
    "# Cria as labels a partir do tópico mais relevante de cada artigo\n",
    "labels = np.argmax(W_nmf, axis=1)\n",
    "\n",
    "# Cria o novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Apresenta os resultados\n",
    "print(df.sort_values(by='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou do resultado? Percebeu que o NMF não só encontrou uma representação em tópicos dos documentos, mas também teve um papel de agregador? Ele realizou um ótimo trabalho em encontrar clusters! E o melhor, nós podemos explicar com palavras o que representa cada um dos tópicos/clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Vamos praticar um pouco com o NMF. O dataset que vamos utilizar é um dataset padrão do scikit-learn que é muito útil para algoritmos de NLP. O dataset contém grupos de discussão no [Usenet](https://en.wikipedia.org/wiki/Usenet) com 18.000 postagens e 20 tópicos principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importe o dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Vamos escolher apenas algumas categorias para facilitar\n",
    "categories = ['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Pegamos apenas o corpo do texto\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, categories=categories,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Para limitar um pouco a quantidade de dados, vamos limitar o dataset\n",
    "data = dataset.data[:2000]\n",
    "targets = dataset.target[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has anybody noticed that Toyota has an uncanny knack for designing horrible\n",
      "ugly station wagons?  Tercels, Corollas, Camrys.  Have their designers no\n",
      "aesthetic sense at all?\n"
     ]
    }
   ],
   "source": [
    "# Exemplos de documentos\n",
    "print(\"\\n\".join(data[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que os dados estão um pouco sujos e tem diversas palavras que podem não significar muito para nós.\n",
    "\n",
    "Crie o vetor de Bag-of-Words utilizando Tf-Idf a partir dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Crie a matriz de frequência de palavras (A) utilizando Tf-Idf\n",
    "# Obs: use um vocabulário de tamanho 10000\n",
    "# Obs 2: é necessário remover as stopwords (dica: utilize stop_words='english')\n",
    "# Obs 3: utilize max_df=0.8 e min_df=2\n",
    "tfidf = _\n",
    "A = _\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar NMF para encontrar os tópicos dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Primeiro, importe o módulo NMF do scikit-learn\n",
    "_\n",
    "\n",
    "# Precisamos criar a instância do NMF\n",
    "# Temos que definir um número de componentes para o NMF\n",
    "# Como temos 4 tópicos temas principais, vamos escolher n_components=4\n",
    "nmf = _\n",
    "\n",
    "# Aplique a transformação NMF\n",
    "W_nmf = _\n",
    "\n",
    "# Vamos ver qual é a dimensão de W_nmf\n",
    "print(W_nmf.shape)\n",
    "\n",
    "# E também a dimensão de H_nmf\n",
    "H_nmf = _\n",
    "print(H_nmf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = _\n",
    "\n",
    "# Vamos criar um dataframe para visualizar os componentes\n",
    "# Dica: Matriz de componentes (atributos) = H_nmf\n",
    "components_df = _\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(4):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest())\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual foi o resultado? Podemos perceber que tópicos foram encontrados? As palavras dentro dos tópicos são coerentes com o que esperávamos?\n",
    "\n",
    "Pegue o exemplo de um documento e veja quais tópicos principais ele contém."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Escolha um índice do documento\n",
    "document_index = _\n",
    "\n",
    "# Veja o documento escolhido\n",
    "print(\"Documento: \\n\", data[document_index])\n",
    "\n",
    "# Apresente os principais tópicos do documento\n",
    "topics = _\n",
    "print(\"\\nTópicos\")\n",
    "print(\"---------\")\n",
    "for i in range(len(topics)):\n",
    "    print(\"Tópico {}: {:.4f}\".format(i, topics[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer como no exemplo do Wikipedia e mostrar a relação entre os temas reais dos documentos e os tópicos encontrados pelo algoritmo NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Topicos reais\n",
    "topics_real = [dataset.target_names[i] for i in targets]\n",
    "\n",
    "# TODO\n",
    "# Crie as labels (tópicos) a partir do tópico mais relevante de cada documento\n",
    "labels = _\n",
    "\n",
    "\n",
    "# Crie um dataframe que una as labels e os tópicos reais\n",
    "df = pd.DataFrame({'labels':labels, 'topics_real':topics_real})\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = pd.crosstab(df['labels'], df['topics_real'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos afirmar que o algoritmo conseguiu clusterizar bem os documentos nos tópicos reais?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Assim como o NMF, o [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) é um algoritmo relativamente simples (pelo menos algoritmicamente) e poderoso para conseguir encontrar tópicos dentro de documentos. Entretanto, diferentemente do NMF, o LDA se baseia em métodos probabilísticos. Mais especificamente, o **LDA** assume que cada **documento** foi gerado a partir de uma **mistura de tópicos** com uma distribuição de probabilidade (no caso, uma distribuição [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution)), e cada **tópico** foi gerado por uma distribuição de **palavras** com certa probabilidade associada (distribuição [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution)).\n",
    "\n",
    "Para imaginar um cenário, imagine que temos os seguintes documentos (retirados deste [link](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)):\n",
    "1. I ate a banana and spinach smoothie for breakfast\n",
    "2. I like to eat broccoli and bananas.\n",
    "3. Chinchillas and kittens are cute.\n",
    "4. My sister adopted a kitten yesterday.\n",
    "5. Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "Para o LDA, os documentos poderiam ser formados pelos seguintes tópicos e palavras:\n",
    "- Documentos 1 e 2: 100% Tópico A\n",
    "- Documentos 3 e 4: 100% Tópico B\n",
    "- Documento 5: 60% Tópico A, 40% Tópico B\n",
    "\n",
    "- Tópico A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching (Talvez represente algo relacionado a comida)\n",
    "- Tópico B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster (Talvez seja relacionado a animais fofinhos)\n",
    "\n",
    "Esse comportamento do LDA é interessante, já que temos uma representação mais intuitiva da formação dos documentos. Poderíamos até criar documentos artificiais a partir dessas definições.\n",
    "\n",
    "Agora vamos ver um exemplo de aplicação utilizando o dataset do Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 15000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anteriormente, nós já realizamos a criação da matriz de frequência de palavras\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O LDA não pode ser utilizado com Tf-Idf, porque ele precisa da contagem total de palavras nos documentos. Por esse modo, vamos utilizar o método [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do scikit-learn. Esse método apenas realiza a contagem de frequência de palavras nos documentos. Para evitar que \"stopwords\" tenham uma importância indevida, podemos remove-las do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raphael\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58, 6)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agora vamos importar o módulo LDA do scikit-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Importa o módulo CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Aplica o método\n",
    "tf = CountVectorizer(max_df=0.95, min_df=0.01, max_features=15000, stop_words='english')\n",
    "\n",
    "X_tf = tf.fit_transform(articles)\n",
    "\n",
    "# Precisamos criar a instância do LDA\n",
    "# Temos que definir um número de tópicos do LDA\n",
    "# Como temos 6 clusters, vamos escolher n_topics=6\n",
    "lda = LatentDirichletAllocation(n_topics=6)\n",
    "\n",
    "\n",
    "# Agora vamos utilizar os mesmos atributos fit, transform ou fit_transform\n",
    "# que já conhecemos do universo do sklearn\n",
    "X_lda = lda.fit_transform(X_tf)\n",
    "\n",
    "# Vamos ver qual é a dimensão de X_lda\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas se manteve em 58, que é o número de documentos (artigos) que nós temos, e o número de colunas se transformou em 6, que é o número de tópicos que nós escolhemos. \n",
    "\n",
    "Vamos agora achar a matriz de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 15000)\n"
     ]
    }
   ],
   "source": [
    "lda_components = lda.components_\n",
    "print(lda_components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o número de linhas é igual ao número de tópicos e o número de colunas representa o número de palavras no nosso vocabulário. Essa matriz é semelhante a matriz de atributos do NMF.\n",
    "\n",
    "Vamos achar quais são as palavras mais importantes para cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topico 0:\n",
      "----------\n",
      "football    264.920863\n",
      "ronaldo     244.128830\n",
      "goal        238.960245\n",
      "scored      225.243590\n",
      "team        203.172803\n",
      "Name: 0, dtype: float64\n",
      "----------\n",
      "Topico 1:\n",
      "----------\n",
      "internet    183.319293\n",
      "explorer    157.738220\n",
      "search      125.726784\n",
      "google      118.780949\n",
      "cookies     112.857214\n",
      "Name: 1, dtype: float64\n",
      "----------\n",
      "Topico 2:\n",
      "----------\n",
      "band        488.264818\n",
      "album       388.483183\n",
      "film        244.027402\n",
      "released    213.281207\n",
      "new         182.892970\n",
      "Name: 2, dtype: float64\n",
      "----------\n",
      "Topico 3:\n",
      "----------\n",
      "fever          49.150908\n",
      "temperature    30.807463\n",
      "doxycycline    23.636070\n",
      "body           18.329051\n",
      "heat           12.797126\n",
      "Name: 3, dtype: float64\n",
      "----------\n",
      "Topico 4:\n",
      "----------\n",
      "climate    133.783280\n",
      "global      91.610138\n",
      "arsenal     91.310688\n",
      "warming     83.053249\n",
      "change      74.470537\n",
      "Name: 4, dtype: float64\n",
      "----------\n",
      "Topico 5:\n",
      "----------\n",
      "emissions    146.974036\n",
      "kyoto         85.193861\n",
      "climate       83.064650\n",
      "countries     78.730527\n",
      "parties       76.895012\n",
      "Name: 5, dtype: float64\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = [x[0] for x in sorted(tf.vocabulary_.items())]\n",
    "\n",
    "# Vamos criar um dataframe para visualizar\n",
    "components_df = pd.DataFrame(lda_components, columns=words)\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(6):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest())\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou da distribuição de palavras dentro de cada tópico? Acha que faz sentido com os temas principais dos artigos do Wikipedia? Cada um dos tópicos poderia ser associado a um cluster?\n",
    "\n",
    "Podemos ainda verificar quais são os tópicos principais de alguns artigos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000132\n",
      "1    0.000131\n",
      "2    0.999345\n",
      "3    0.000130\n",
      "4    0.000131\n",
      "5    0.000131\n",
      "Name: Denzel Washington, dtype: float64\n",
      "\n",
      "0    0.000091\n",
      "1    0.000092\n",
      "2    0.999542\n",
      "3    0.000092\n",
      "4    0.000092\n",
      "5    0.000091\n",
      "Name: Leukemia, dtype: float64\n",
      "\n",
      "0    0.999661\n",
      "1    0.000068\n",
      "2    0.000068\n",
      "3    0.000068\n",
      "4    0.000068\n",
      "5    0.000068\n",
      "Name: Neymar, dtype: float64\n",
      "\n",
      "0    0.000143\n",
      "1    0.999288\n",
      "2    0.000143\n",
      "3    0.000142\n",
      "4    0.000142\n",
      "5    0.000143\n",
      "Name: LinkedIn, dtype: float64\n",
      "\n",
      "0    0.000070\n",
      "1    0.000070\n",
      "2    0.999651\n",
      "3    0.000070\n",
      "4    0.000070\n",
      "5    0.000070\n",
      "Name: Arctic Monkeys, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Precisamos criar um dataframe para facilitar nossa vida\n",
    "df = pd.DataFrame(X_lda, index=titles)\n",
    "\n",
    "print(df.loc['Denzel Washington'])\n",
    "print()\n",
    "print(df.loc['Leukemia'])\n",
    "print()\n",
    "print(df.loc['Neymar'])\n",
    "print()\n",
    "print(df.loc['LinkedIn'])\n",
    "print()\n",
    "print(df.loc['Arctic Monkeys'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar alguma relação de proximidade entre os tópicos?\n",
    "\n",
    "Vamos agrupar agora os artigos pelos tópicos principais de cada um deles e ver como eles tém relação com os clusters definidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          article  label\n",
      "28                  France national football team      0\n",
      "54              2014 FIFA World Cup qualification      0\n",
      "53                                  Franck Ribéry      0\n",
      "44                              Cristiano Ronaldo      0\n",
      "6                                        Football      0\n",
      "36                                 Radamel Falcao      0\n",
      "11                             Zlatan Ibrahimović      0\n",
      "26                Colombia national football team      0\n",
      "17                                         Neymar      0\n",
      "55                                 Alexa Internet      1\n",
      "51                                       LinkedIn      1\n",
      "40                              Internet Explorer      1\n",
      "39                                       HTTP 404      1\n",
      "56                                  Google Search      1\n",
      "23                                  Social search      1\n",
      "22                                        Firefox      1\n",
      "57                                         Tumblr      1\n",
      "3                                     HTTP cookie      1\n",
      "0                                   Black Sabbath      2\n",
      "49                                    Tonsillitis      2\n",
      "5                            Catherine Zeta-Jones      2\n",
      "50                               Jennifer Aniston      2\n",
      "42                                   Chad Kroeger      2\n",
      "7                               Denzel Washington      2\n",
      "8                                     Hepatitis B      2\n",
      "38                                  Anne Hathaway      2\n",
      "16                                     The Wanted      2\n",
      "52                                     Prednisone      2\n",
      "2                                     Hepatitis C      2\n",
      "46                             Michael Fassbender      2\n",
      "31                                       Leukemia      2\n",
      "32                                         Sepsis      2\n",
      "29                          Red Hot Chili Peppers      2\n",
      "10                                   Jessica Biel      2\n",
      "27                                     Nate Ruess      2\n",
      "12                                  Russell Crowe      2\n",
      "25                                 Dakota Fanning      2\n",
      "13                                       Skrillex      2\n",
      "14                                 Arctic Monkeys      2\n",
      "21                                     Gabapentin      2\n",
      "20                                   Stevie Nicks      2\n",
      "19                                 Angelina Jolie      2\n",
      "18                                     Mila Kunis      2\n",
      "1                                        Lymphoma      2\n",
      "34                                    Doxycycline      3\n",
      "24                                          Fever      3\n",
      "4                                  Global warming      4\n",
      "48                                   Arsenal F.C.      4\n",
      "15                                 Climate change      4\n",
      "9                                            Gout      4\n",
      "35                                   Nigel Lawson      4\n",
      "45  2010 United Nations Climate Change Conference      5\n",
      "43  2007 United Nations Climate Change Conference      5\n",
      "41                                 Kyoto Protocol      5\n",
      "33                                        350.org      5\n",
      "30                               Connie Hedegaard      5\n",
      "47       Nationally Appropriate Mitigation Action      5\n",
      "37  Greenhouse gas emissions by the United States      5\n"
     ]
    }
   ],
   "source": [
    "# Cria as labels a partir do tópico mais relevante de cada artigo\n",
    "labels = np.argmax(X_lda, axis=1)\n",
    "\n",
    "# Cria o novo dataframe com os labels dos clusters\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Apresenta os resultados\n",
    "print(df.sort_values(by='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que achou do resultado? Talvez o LDA não tenha funcionado exatamente do jeito que acreditávamos que ele iria funcionar, certo? Isso não quer dizer que os tópicos encontrados por ele não sejam corretos, mas apenas que são diferentes do que experaríamos para o dado problema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Agora vamos praticar o LDA com o dataset 20newsgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importe o dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Vamos escolher apenas algumas categorias para facilitar\n",
    "categories = ['rec.autos', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Pegamos apenas o corpo do texto\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, categories=categories,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Para limitar um pouco a quantidade de dados, vamos limitar o dataset\n",
    "data = dataset.data[:2000]\n",
    "targets = dataset.target[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has anybody noticed that Toyota has an uncanny knack for designing horrible\n",
      "ugly station wagons?  Tercels, Corollas, Camrys.  Have their designers no\n",
      "aesthetic sense at all?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplos de documentos\n",
    "print(\"\\n\".join(data[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que os dados estão um pouco sujos e tem diversas palavras que podem não significar muito para nós.\n",
    "\n",
    "Crie o vetor de Bag-of-Words utilizando Term Frequency (CountVectorizer) a partir dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Crie a matriz de frequência de palavras (A) utilizando Tf\n",
    "# Obs: use um vocabulário de tamanho 10000\n",
    "# Obs 2: é necessário remover as stopwords (dica: utilize stop_words='english')\n",
    "# Obs 3: utilize max_df=0.8 e min_df=2\n",
    "tf = _\n",
    "A = _\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar LDA para encontrar os tópicos dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Primeiro, importe o módulo LDA do scikit-learn\n",
    "_\n",
    "\n",
    "# Precisamos criar a instância do LDA\n",
    "# Temos que definir um número de componentes para o LDA\n",
    "# Como temos 4 tópicos temas principais, vamos escolher n_topics=4\n",
    "lda = _\n",
    "\n",
    "# Aplique a transformação LDA\n",
    "X_lda = _\n",
    "\n",
    "# Vamos ver qual é a dimensão de X_lda\n",
    "print(X_lda.shape)\n",
    "\n",
    "# Também veremos quais são os componentes do LDA\n",
    "lda_components = _\n",
    "print(lda_components.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Precisamos criar uma lista de palavras que representam as \n",
    "# colunas da matriz de frequência de palavras\n",
    "words = _\n",
    "\n",
    "# Vamos criar um dataframe para visualizar os componentes\n",
    "# Dica: Matriz de componentes (atributos) = lda_components\n",
    "components_df = _\n",
    "\n",
    "# Vamos verificar as palavras que representam cada um dos tópicos\n",
    "for i in range(4):\n",
    "    component = components_df.iloc[i]\n",
    "    print(\"Topico {}:\".format(i))\n",
    "    print(\"----------\")\n",
    "    print(component.nlargest())\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual foi o resultado? Podemos perceber que tópicos foram encontrados? As palavras dentro dos tópicos são coerentes com o que esperávamos?\n",
    "\n",
    "Pegue o exemplo de um documento e veja quais tópicos principais ele contém."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Escolha um índice do documento\n",
    "document_index = _\n",
    "\n",
    "# Veja o documento escolhido\n",
    "print(\"Documento: \\n\", data[document_index])\n",
    "\n",
    "# Apresente os principais tópicos do documento\n",
    "topics = _\n",
    "print(\"\\nTópicos\")\n",
    "print(\"---------\")\n",
    "for i in range(len(topics)):\n",
    "    print(\"Tópico {}: {:.4f}\".format(i, topics[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer como no exemplo do Wikipedia e mostrar a relação entre os temas reais dos documentos e os tópicos encontrados pelo algoritmo LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Topicos reais\n",
    "topics_real = [dataset.target_names[i] for i in targets]\n",
    "\n",
    "# TODO\n",
    "# Crie as labels (tópicos) a partir do tópico mais relevante de cada documento\n",
    "labels = _\n",
    "\n",
    "\n",
    "# Crie um dataframe que una as labels e os tópicos reais\n",
    "df = _\n",
    "\n",
    "# Crie a matriz de tabulação cruzada\n",
    "ct = _\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos afirmar que o algoritmo conseguiu clusterizar bem os documentos nos tópicos reais?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case\n",
    "\n",
    "Esse case foi retirado desse [artigo](https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac). O objetivo dele é verificar se conseguimos extrair tópicos relevantes de um dataset contendo 8.447 matérias do NY Times, com um vocabulário de 3.012 palavras. Por razão de direitos autorias, os documentos não possuem título.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1400/1*toWf7lAVf_5GIb9IMfS8Bw.jpeg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo `nyt_data.txt` contém os documentos, onde cada linha representa um documento específico. O dataset já sofreu um processo de limpeza e vetorização, onde foram mantidas apenas palavras que ocorreram mais de 10 vezes. Cada palavra é representada por um índice, que pode ser acessado pelo arquivo `nyt_vocab.txt`, e por sua frequência no documento.\n",
    "\n",
    "Vamos criar a matriz de frequências a partir desses dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contém os documentos do dataset:\n",
    "# Cada linha representa um documento\n",
    "# Cada documento contém um conjunto de índices de palavras e \n",
    "# a sua frequência no documento -> word_idx:term_freq\n",
    "with open(os.path.join(DATASET_FOLDER, 'nyt_data.txt')) as f:\n",
    "    documents = f.readlines()\n",
    "documents = [x.strip().strip('\\n').strip(\"'\") for x in documents] \n",
    "\n",
    "# Contém o vocabulário do dataset:\n",
    "# Cada linha representa o índice da palavra\n",
    "with open(os.path.join(DATASET_FOLDER, 'nyt_vocab.txt')) as f:\n",
    "    vocabs = f.readlines()\n",
    "vocabs = [x.strip().strip('\\n').strip(\"'\") for x in vocabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar um exemplo de documento e vocabulários:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 0:\n",
      " 1946:2,1168:2,1194:2,1275:1,777:1,522:1,107:1,839:2,424:2,2330:2,1878:2,344:1,1008:1,94:3,735:1,212:1,2407:1,2623:1,781:2,42:1,160:1,1141:1,117:1,16:1,108:1,153:2,1137:2,416:1,23:2,46:1,734:2,284:1,207:2,301:1,357:1,780:2,2564:2,206:1,106:2,892:1,272:1,1557:1,2003:1,1079:2,370:1,1894:2,266:1,143:1,1532:1,2551:1,223:2,30:1,1509:1,1921:2,361:2,311:1,1549:2,203:1,2821:1,546:2,1612:1,200:1,247:2,1731:2,565:1,2253:1,234:1,72:1,648:1,1072:1,518:1,39:1,703:2,625:1,140:1,2301:1,32:1,462:1,743:1,2017:2,925:1,118:1,1000:2,836:1,161:1,942:2,885:1,267:1,2683:1,626:1,317:1,69:1,860:2,633:1,2658:1,75:2,1563:1,690:1,802:1,1650:1,1836:1,111:1,2151:1,128:2,2864:1,22:2,1301:1,250:2,1922:2,936:1,918:1,775:1,280:1,18:2,182:1,667:1,2383:1,878:1,1498:1,109:2,1577:1,1758:1,60:1,2269:1,215:1,2038:2,485:1,335:1,2543:2,422:1,12:1,585:2,1754:1,1293:1,604:1,52:1,248:1,2:2,603:2,1345:2,271:3,668:1,1022:2,260:1,1251:2,498:1,2213:1,351:3,2584:1,1349:1,334:2,218:1,34:1,256:1,2576:2,58:1,1637:1,926:2,313:1,2041:2,1505:1,1698:2,587:1\n",
      "\n",
      "Parte do vocabulário:\n",
      " ['company', 'percent', 'state', 'play', 'official', 'game', 'man', 'city', 'plan', 'school']\n"
     ]
    }
   ],
   "source": [
    "print(\"Documento 0:\\n\", documents[0])\n",
    "print(\"\\nParte do vocabulário:\\n\", vocabs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar a matriz de frequência de termos **A** a partir dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8447, 3012)\n"
     ]
    }
   ],
   "source": [
    "# Número de documentos\n",
    "n_doc = 8447\n",
    "# Número de palavras\n",
    "n_words = 3012 \n",
    "A = np.zeros([n_doc,n_words])\n",
    "\n",
    "for j in range(len(documents)):\n",
    "    for i in documents[j].split(','):\n",
    "        word, freq = i.split(':')\n",
    "        A[j,int(word)-1] = int(freq)\n",
    "        \n",
    "# Tamanho da matriz de frequência de termos:\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora estamos prontos para realizar o processo de topic analysis! Divirta-se!\n",
    "\n",
    "*Dica: Utilize entre 20 e 25 tópicos*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Boa sorte =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema de recomendação\n",
    "\n",
    "Em poucos anos a internet revolucionou o mercado de consumo mundial e a forma como os clientes interagem com os vendedores. Uma das dinâmicas criadas mais importantes a partir dessa revolução é a de sistemas de recomendação. Todos devem já ter notado o quanto e-commerces como a Amazon, Best Buy e até empresas brasileiras acertam ao recomendar certos tipos de produto para seus clientes, que muitas vezes nem os estavam procurando (veja esse [artigo](https://www.techemergence.com/use-cases-recommendation-systems/)). Isso não acontece mais apenas em e-commerces, mas também com provedores de música (Spotify), de filmes (Netflix) ou vídeos em geral (Youtube). \n",
    "\n",
    "Esse fenômeno só ocorre devido a um fator: **dado**. As empresas atualmente possuem muita informação sobre os seus produtos e os seus usuários. É muito fácil obter, hoje em dia, os interesses dos clientes sobre seus produtos. Seja o fato de o vendedor comprar um produto, dar um review ou apenas clicar, já é suficiente para uma empresa mapear os interesses dos usuários e tentar direcionar produtos que seriam relevantes para o usuário sem nem mesmo ele saber!\n",
    "\n",
    "Os sistemas de recomendação se baseiam, basicamente, em encontrar relações entre compradores e produtos. Mais especificamente, existem dois grandes grupos de sistemas de recomendação:\n",
    "- **Proximidade de produtos**: Tem o objetivo de encontrar produtos similares aos consumidos por um cliente. Se um cliente possui o interesse em um determinado produto, o sistema de recomendação pode tentar encontrar outros produtos similares para indicar para o cliente.\n",
    "- **Proximidade entre clientes** (Filtro Colaborativo): Tem o objetivo de encontrar clientes com interesses semelhantes. Suponha que exista um cliente X que consuma os produtos A e B, enquanto um outro cliente Y tem interesse nos produtos A, B e C. Como eles possuem interesses semelhantes (produtos A e B), o sistema de recomendação poderia indicar o produto C para o cliente X.\n",
    "\n",
    "Existem diversos outros tipos de sistemas de recomendação que fogem do escopo desse material. Mais informações podem ser vistas nesse [link](https://www.techemergence.com/use-cases-recommendation-systems/).\n",
    "\n",
    "##### Exemplo\n",
    "Agora vamos tentar criar um sistema simples de recomendação baseado em **proximidade de produtos**! \n",
    "\n",
    "Vamos utilizar o dataset de artigos do NY Times para essa tarefa. Vamos supor que uma pessoa tenha lido um determinado artigo e nós gostaríamos de recomendar outros artigos semelhantes àquele. Lembre que para comparar dois documentos que contém vetores de atributos associados a palavras, a melhor medida de distância é a **distância de cossenos**.\n",
    "\n",
    "Para calcular a distância entre produtos, nós poderíamos utilizar todo o espaço de atributos (quantidade de palavras no vocabulário), mas isso é desnecessário. Nós temos uma representação resumida de cada documento por sua proporção de tópicos, que formam o novo vetor de atributos. Como já encontramos os tópicos relacionados aos documentos no exercício anterior, podemos aproveitá-los para resolver nosso problema atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nós vamos ter que normalizar o vetor de atributos\n",
    "# Isso é necessário para que todas as features (tópicos) \n",
    "# de um documento some 1 ao final\n",
    "# Seria a porcentagem de composição dos tópicos \n",
    "# no documento\n",
    "from sklearn.preprocessing import normalize\n",
    "W_nmf_norm = normalize(W_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic 15</th>\n",
       "      <th>topic 16</th>\n",
       "      <th>topic 17</th>\n",
       "      <th>topic 18</th>\n",
       "      <th>topic 19</th>\n",
       "      <th>topic 20</th>\n",
       "      <th>topic 21</th>\n",
       "      <th>topic 22</th>\n",
       "      <th>topic 23</th>\n",
       "      <th>topic 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc 0</th>\n",
       "      <td>0.294146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303896</td>\n",
       "      <td>0.517961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153861</td>\n",
       "      <td>0.025917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc 1</th>\n",
       "      <td>0.316441</td>\n",
       "      <td>0.546551</td>\n",
       "      <td>0.048343</td>\n",
       "      <td>0.017302</td>\n",
       "      <td>0.139095</td>\n",
       "      <td>0.279719</td>\n",
       "      <td>0.159308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.102785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095460</td>\n",
       "      <td>0.057694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653904</td>\n",
       "      <td>0.018420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc 2</th>\n",
       "      <td>0.524771</td>\n",
       "      <td>0.037032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.791191</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc 3</th>\n",
       "      <td>0.431515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130489</td>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.099081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.366686</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.034564</td>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.070738</td>\n",
       "      <td>0.150782</td>\n",
       "      <td>0.154748</td>\n",
       "      <td>0.200475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc 4</th>\n",
       "      <td>0.024630</td>\n",
       "      <td>0.229870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009689</td>\n",
       "      <td>0.049841</td>\n",
       "      <td>0.121191</td>\n",
       "      <td>0.041780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.054078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053154</td>\n",
       "      <td>0.225243</td>\n",
       "      <td>0.084851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic 0   topic 1   topic 2   topic 3   topic 4   topic 5   topic 6  \\\n",
       "doc 0  0.294146  0.000000  0.000000  0.000000  0.002036  0.000000  0.000000   \n",
       "doc 1  0.316441  0.546551  0.048343  0.017302  0.139095  0.279719  0.159308   \n",
       "doc 2  0.524771  0.037032  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "doc 3  0.431515  0.000000  0.130489  0.027181  0.099081  0.000000  0.068598   \n",
       "doc 4  0.024630  0.229870  0.000000  0.009689  0.049841  0.121191  0.041780   \n",
       "\n",
       "        topic 7   topic 8   topic 9    ...     topic 15  topic 16  topic 17  \\\n",
       "doc 0  0.303896  0.517961  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "doc 1  0.000000  0.020161  0.102785    ...     0.000000  0.012730  0.000000   \n",
       "doc 2  0.000000  0.000000  0.000000    ...     0.791191  0.040254  0.000000   \n",
       "doc 3  0.018393  0.366686  0.025766    ...     0.117798  0.002651  0.034564   \n",
       "doc 4  0.000000  0.013547  0.054078    ...     0.053154  0.225243  0.084851   \n",
       "\n",
       "       topic 18  topic 19  topic 20  topic 21  topic 22  topic 23  topic 24  \n",
       "doc 0  0.000000  0.082041  0.000000  0.000000  0.000000  0.153861  0.025917  \n",
       "doc 1  0.000000  0.095460  0.057694  0.000000  0.000000  0.653904  0.018420  \n",
       "doc 2  0.000000  0.161058  0.000000  0.000000  0.217096  0.000000  0.000000  \n",
       "doc 3  0.024858  0.003601  0.070738  0.150782  0.154748  0.200475  0.000000  \n",
       "doc 4  0.000000  0.130305  0.000000  0.421757  0.000000  0.000000  0.092667  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos criar um dataframe:\n",
    "# índice: documento\n",
    "# colunas: vetor de features (normalizadas)\n",
    "df = pd.DataFrame(W_nmf_norm,\n",
    "                  columns=['topic {}'.format(i) for i in range(n_topics)],\n",
    "                  index=['doc {}'.format(i) for i in range(n_doc)])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7618"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos escolher um documento aleatoriamente\n",
    "# Esse documento será o escolhido pelo cliente\n",
    "doc_target = np.random.choice(range(len(documents)))\n",
    "doc_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0     0.133103\n",
      "topic 1     0.575644\n",
      "topic 2     0.051933\n",
      "topic 3     0.000000\n",
      "topic 4     0.550594\n",
      "topic 5     0.025231\n",
      "topic 6     0.424973\n",
      "topic 7     0.139334\n",
      "topic 8     0.206514\n",
      "topic 9     0.107456\n",
      "topic 10    0.000000\n",
      "topic 11    0.000000\n",
      "topic 12    0.161399\n",
      "topic 13    0.000000\n",
      "topic 14    0.000000\n",
      "topic 15    0.000000\n",
      "topic 16    0.140682\n",
      "topic 17    0.000000\n",
      "topic 18    0.000000\n",
      "topic 19    0.000000\n",
      "topic 20    0.000000\n",
      "topic 21    0.135645\n",
      "topic 22    0.154468\n",
      "topic 23    0.000000\n",
      "topic 24    0.046040\n",
      "Name: doc 7618, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verifica o vetor de features dele:\n",
    "doc_target_features = df.iloc[doc_target]\n",
    "print(doc_target_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontre agora os documentos mais próximos a esse documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artigos recomendados:\n",
      "doc 1313    0.911258\n",
      "doc 3083    0.905729\n",
      "doc 4974    0.901671\n",
      "doc 3370    0.883303\n",
      "doc 5165    0.870204\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calcula a distância de cossenos entre o artigo e \n",
    "# todos os outros documentos\n",
    "cossine_distance = df.dot(doc_target_features)\n",
    "\n",
    "# Todos os documentos com maior distância de cossenos \n",
    "# representam os documentos mais próximos\n",
    "print(\"Artigos recomendados:\")\n",
    "recommendations = cossine_distance.nlargest(6)[1:]\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar os principais tópicos desses documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artigo original: doc 7618\n",
      "-----------------------\n",
      "- Tópicos (palavras): \n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "        topic 4 (city, building, area, build, house)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "\n",
      "\n",
      "\n",
      "Recomendação doc 1313:\n",
      "-----------------------\n",
      "- Similaridade: 91.13%\n",
      "- Tópicos (palavras): \n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "        topic 4 (city, building, area, build, house)\n",
      "\n",
      "\n",
      "Recomendação doc 3083:\n",
      "-----------------------\n",
      "- Similaridade: 90.57%\n",
      "- Tópicos (palavras): \n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "        topic 4 (city, building, area, build, house)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "\n",
      "\n",
      "Recomendação doc 4974:\n",
      "-----------------------\n",
      "- Similaridade: 90.17%\n",
      "- Tópicos (palavras): \n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "       topic 22 (art, artist, exhibition, museum, painting)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "\n",
      "\n",
      "Recomendação doc 3370:\n",
      "-----------------------\n",
      "- Similaridade: 88.33%\n",
      "- Tópicos (palavras): \n",
      "        topic 4 (city, building, area, build, house)\n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "\n",
      "\n",
      "Recomendação doc 5165:\n",
      "-----------------------\n",
      "- Similaridade: 87.02%\n",
      "- Tópicos (palavras): \n",
      "        topic 4 (city, building, area, build, house)\n",
      "        topic 1 (thing, tell, ask, feel, lot)\n",
      "        topic 6 (pay, money, state, budget, tax)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_top_topics = df.iloc[doc_target].nlargest(3).index.values\n",
    "default_components = [components_df.iloc[int(i.split(' ')[1])].nlargest(5).index.values \n",
    "                      for i in default_top_topics]\n",
    "\n",
    "print(\"Artigo original: doc {}\".format(doc_target))\n",
    "print(\"-----------------------\")\n",
    "print(\"- Tópicos (palavras): \")\n",
    "for i in range(len(top_topics)):\n",
    "    print(\"{:>15} ({})\".format(default_top_topics[i], ', '.join(default_components[i])))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for doc, similarity in recommendations.items():\n",
    "    doc_num = int(doc.split(' ')[1])\n",
    "    top_topics = df.iloc[doc_num].nlargest(3).index.values\n",
    "    components = [components_df.iloc[int(i.split(' ')[1])].nlargest(5).index.values for i in top_topics]\n",
    "    print(\"Recomendação {}:\".format(doc))\n",
    "    print(\"-----------------------\")\n",
    "    print(\"- Similaridade: {:.2%}\".format(similarity))\n",
    "    print(\"- Tópicos (palavras): \")\n",
    "    for i in range(len(top_topics)):\n",
    "        print(\"{:>15} ({})\".format(top_topics[i], ', '.join(components[i])))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que os tópicos são realmente semelhantes. Poderíamos recomendar esses produtos para o cliente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
